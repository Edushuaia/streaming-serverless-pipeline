# ğŸ“¸ GuÃ­a para Capturar Evidencias y Mostrarlas en la Web

## ğŸ¯ Objetivo

Ejecutar el proyecto en GCP, capturar evidencias de cada servicio, y mostrarlas profesionalmente en tu portafolio web.

---

## ğŸ“‹ FASE 1: PreparaciÃ³n del Entorno

### Paso 1.1: Verificar que tienes todo configurado

```bash
# En tu terminal (con venv activado)
cd /Users/ashramsatcitananda/Desktop/streaming-serverless-pipeline

# Verificar archivo .env existe
ls -la .env

# Si NO existe, crÃ©alo
cp .env.example .env
nano .env  # Edita con tus valores de GCP
```

### Paso 1.2: Autenticarte en GCP

```bash
# AutenticaciÃ³n
gcloud auth login

# Configurar proyecto
gcloud config set project streaming-serverless-dataflow

# Verificar autenticaciÃ³n
gcloud auth list
gcloud config get-value project
```

---

## ğŸ—ï¸ FASE 2: Crear Infraestructura en GCP

### Paso 2.1: Crear Topic de Pub/Sub

```bash
# Crear el topic
gcloud pubsub topics create transactions-topic

# Crear la subscription para Dataflow
gcloud pubsub subscriptions create dataflow-subscription \
  --topic=transactions-topic \
  --ack-deadline=60

# Verificar creaciÃ³n
gcloud pubsub topics list
gcloud pubsub subscriptions list
```

#### ğŸ“¸ CAPTURA 1: Console de Pub/Sub

- Ve a: <https://console.cloud.google.com/cloudpubsub/topic/list>
- Captura pantalla mostrando:
  - âœ… Topic `transactions-topic` creado
  - âœ… Subscription `dataflow-subscription`
  - ğŸ“ Guarda como: `img/evidencias/01-pubsub-topic.png`

### Paso 2.2: Crear Dataset en BigQuery

```bash
# Ejecutar script de configuraciÃ³n
chmod +x setup_bigquery.sh
./setup_bigquery.sh

# O manualmente:
bq mk --dataset \
  --location=us-central1 \
  streaming_data_warehouse_v2

# Crear tabla
bq mk --table \
  streaming_data_warehouse_v2.transaction_aggregates \
  window_start_time:TIMESTAMP,window_end_time:TIMESTAMP,total_transactions:INTEGER,total_amount_sum:FLOAT
```

#### ğŸ“¸ CAPTURA 2: BigQuery Dataset

- Ve a: <https://console.cloud.google.com/bigquery>
- Captura mostrando:
  - âœ… Dataset `streaming_data_warehouse_v2`
  - âœ… Tabla `transaction_aggregates`
  - âœ… Esquema de la tabla visible
  - ğŸ“ Guarda como: `img/evidencias/02-bigquery-dataset.png`

### Paso 2.3: Crear Bucket de Cloud Storage

```bash
# Crear bucket para staging de Dataflow
gsutil mb -l us-central1 gs://streaming-serverless-dataflow-staging

# Verificar
gsutil ls
```

#### ğŸ“¸ CAPTURA 3: Cloud Storage Bucket

- Ve a: <https://console.cloud.google.com/storage/browser>
- Captura el bucket creado
- ğŸ“ Guarda como: `img/evidencias/03-storage-bucket.png`

---

## ğŸš€ FASE 3: Ejecutar el Pipeline

### Paso 3.1: Iniciar Dataflow Job

```bash
# AsegÃºrate de estar en el directorio correcto
cd /Users/ashramsatcitananda/Desktop/streaming-serverless-pipeline

# Ejecutar pipeline en Dataflow (PRODUCCIÃ“N)
python dataflow_pipeline.py \
  --runner DataflowRunner \
  --project streaming-serverless-dataflow \
  --region us-central1 \
  --temp_location gs://streaming-serverless-dataflow-staging/temp/ \
  --staging_location gs://streaming-serverless-dataflow-staging/staging/ \
  --streaming \
  --max_num_workers 5 \
  --num_workers 2
```

**Salida esperada:**

```text
2025-12-17 10:30:00 - dataflow_pipeline - INFO - Iniciando pipeline de streaming...
2025-12-17 10:30:10 - dataflow_pipeline - INFO - Job ID: 2025-12-17_02_30_00-1234567890
2025-12-17 10:30:15 - dataflow_pipeline - INFO - Job URL: https://console.cloud.google.com/dataflow/jobs/...
```

#### ğŸ“¸ CAPTURA 4: Dataflow Job en EjecuciÃ³n

- Ve a: <https://console.cloud.google.com/dataflow/jobs>
- Espera 2-3 minutos hasta que el job estÃ© "Running"
- Captura mostrando:
  - âœ… Job status: Running (verde)
  - âœ… Graph del pipeline visible
  - âœ… Workers activos
  - ğŸ“ Guarda como: `img/evidencias/04-dataflow-running.png`

#### ğŸ“¸ CAPTURA 5: Grafo del Pipeline

- En la misma pÃ¡gina, haz clic en la pestaÃ±a "Job graph"
- Captura el grafo completo mostrando:
  - ReadFromPubSub
  - ParseJson
  - Window
  - Aggregate
  - WriteToBigQuery
- ğŸ“ Guarda como: `img/evidencias/05-dataflow-graph.png`

---

## ğŸ“¤ FASE 4: Publicar Mensajes de Prueba

### Paso 4.1: Ejecutar el Simulador

**En una NUEVA terminal** (deja Dataflow corriendo):

```bash
# Activar entorno virtual
cd /Users/ashramsatcitananda/Desktop/streaming-serverless-pipeline
source venv/bin/activate

# Ejecutar simulador (publica 100 mensajes)
python publisher_simulator.py --num-messages 100 --interval 0.5
```

**Salida esperada:**

```text
============================================================
INICIANDO SIMULADOR DE TRANSACCIONES
============================================================
Topic: projects/streaming-serverless-dataflow/topics/transactions-topic
Intervalo: 0.5s
Max mensajes: 100
============================================================
âœ… Mensaje publicado: TXN-abc123 | Amount: $125.50
âœ… Mensaje publicado: TXN-def456 | Amount: $89.99
...
============================================================
ESTADÃSTICAS FINALES
============================================================
âœ… Total publicados: 100
âŒ Errores: 0
ğŸ“Š Tasa promedio: 2.00 msg/s
â±ï¸  Tiempo total: 50.0s
```

#### ğŸ“¸ CAPTURA 6: Terminal del Simulador

- Captura tu terminal mostrando los mensajes publicados
- ğŸ“ Guarda como: `img/evidencias/06-simulator-output.png`

### Paso 4.2: Verificar Mensajes en Pub/Sub

#### ğŸ“¸ CAPTURA 7: MÃ©tricas de Pub/Sub

- Ve a: <https://console.cloud.google.com/cloudpubsub/topic/detail/transactions-topic>
- PestaÃ±a "Metrics"
- Captura mostrando:
  - ğŸ“ˆ GrÃ¡fica de mensajes publicados
  - ğŸ“Š Throughput
  - ğŸ“ Guarda como: `img/evidencias/07-pubsub-metrics.png`

---

## ğŸ“Š FASE 5: Verificar Resultados en BigQuery

### Paso 5.1: Esperar Procesamiento

```bash
# Espera 2-3 minutos para que los datos se procesen y escriban a BigQuery
# Puedes monitorear en la consola de Dataflow
```

### Paso 5.2: Consultar Datos

```bash
# Desde terminal
bq query --use_legacy_sql=false '
SELECT 
  window_start_time,
  window_end_time,
  total_transactions,
  total_amount_sum,
  ROUND(total_amount_sum / total_transactions, 2) as avg_amount
FROM `streaming-serverless-dataflow.streaming_data_warehouse_v2.transaction_aggregates`
ORDER BY window_start_time DESC
LIMIT 10
'
```

#### ğŸ“¸ CAPTURA 8: Resultados en BigQuery

- Ve a: <https://console.cloud.google.com/bigquery>
- Ejecuta la consulta en el editor SQL
- Captura mostrando:
  - âœ… Resultados de la query (tabla con datos)
  - âœ… Timestamps de las ventanas
  - âœ… Totales agregados
  - ğŸ“ Guarda como: `img/evidencias/08-bigquery-results.png`

#### ğŸ“¸ CAPTURA 9: Esquema de la Tabla

- En BigQuery, haz clic en la tabla `transaction_aggregates`
- PestaÃ±a "Schema"
- Captura el esquema completo
- ğŸ“ Guarda como: `img/evidencias/09-bigquery-schema.png`

---

## ğŸ“ˆ FASE 6: MÃ©tricas y Monitoreo

### Paso 6.1: MÃ©tricas de Dataflow

#### ğŸ“¸ CAPTURA 10: MÃ©tricas de Dataflow

- En la pÃ¡gina del job de Dataflow
- PestaÃ±a "Metrics"
- Captura mostrando:
  - ğŸ“Š Elements added
  - â±ï¸ System lag
  - ğŸ’¾ Throughput
  - ğŸ“ Guarda como: `img/evidencias/10-dataflow-metrics.png`

#### ğŸ“¸ CAPTURA 11: Workers Autoscaling

- PestaÃ±a "Workers"
- Captura mostrando nÃºmero de workers activos
- ğŸ“ Guarda como: `img/evidencias/11-dataflow-workers.png`

### Paso 6.2: Logs y Debugging

#### ğŸ“¸ CAPTURA 12: Logs Estructurados

- Ve a: <https://console.cloud.google.com/logs/query>
- Filtra por el job de Dataflow
- Captura logs mostrando:
  - âœ… Mensajes INFO
  - âœ… Timestamps estructurados
  - âœ… Contexto adicional
  - ğŸ“ Guarda como: `img/evidencias/12-logs.png`

---

## ğŸ§ª FASE 7: Evidencias de Testing

### Paso 7.1: Ejecutar Tests

```bash
# Tests con output detallado
pytest test_pipeline.py -v --tb=short

# Tests con cobertura
pytest test_pipeline.py -v --cov=dataflow_pipeline --cov-report=term-missing --cov-report=html
```

#### ğŸ“¸ CAPTURA 13: Output de Tests

- Captura tu terminal mostrando:
  - âœ… Todos los tests passing
  - ğŸ“Š Cobertura > 80%
  - ğŸ“ Guarda como: `img/evidencias/13-tests-output.png`

#### ğŸ“¸ CAPTURA 14: Reporte de Cobertura HTML

- Abre: `htmlcov/index.html` en tu navegador
- Captura el dashboard de cobertura
- ğŸ“ Guarda como: `img/evidencias/14-coverage-report.png`

---

## ğŸ¨ FASE 8: Implementar en la PÃ¡gina Web

### Paso 8.1: Crear Directorio de Evidencias

```bash
# Crear carpeta para evidencias
mkdir -p img/evidencias

# Mover todas tus capturas ahÃ­
# (Las que guardaste en los pasos anteriores)
```

### Paso 8.2: Crear SecciÃ³n de Evidencias en la Web

Vamos a crear una nueva pÃ¡gina: `evidencias.html`

---

## ğŸ“¸ Checklist Final de Capturas

### Infraestructura (3 capturas)

- [ ] `01-pubsub-topic.png` - Topic y Subscription de Pub/Sub
- [ ] `02-bigquery-dataset.png` - Dataset y tabla en BigQuery
- [ ] `03-storage-bucket.png` - Bucket de Cloud Storage

### Pipeline en EjecuciÃ³n (4 capturas)

- [ ] `04-dataflow-running.png` - Job de Dataflow activo
- [ ] `05-dataflow-graph.png` - Grafo del pipeline
- [ ] `06-simulator-output.png` - Terminal del simulador
- [ ] `07-pubsub-metrics.png` - MÃ©tricas de Pub/Sub

### Resultados (3 capturas)

- [ ] `08-bigquery-results.png` - Datos procesados en BigQuery
- [ ] `09-bigquery-schema.png` - Esquema de la tabla
- [ ] `10-dataflow-metrics.png` - MÃ©tricas del pipeline

### Monitoreo (4 capturas)

- [ ] `11-dataflow-workers.png` - Workers y autoscaling
- [ ] `12-logs.png` - Logs estructurados
- [ ] `13-tests-output.png` - Tests pasando
- [ ] `14-coverage-report.png` - Reporte de cobertura

---

## ğŸš€ Siguiente Paso

Una vez tengas todas las capturas, ejecuta:

```bash
# Verificar que tienes todas las imÃ¡genes
ls -lh img/evidencias/

# DeberÃ­a mostrar 14 archivos PNG
```

Luego te ayudarÃ© a crear la pÃ¡gina `evidencias.html` para mostrarlas profesionalmente.

---

## ğŸ’¡ Tips para Buenas Capturas

1. **ResoluciÃ³n**: Captura en alta resoluciÃ³n (al menos 1920x1080)
2. **Pantalla completa**: Usa el navegador en fullscreen (F11)
3. **Ocular info sensible**: Borra IDs de proyecto si es necesario
4. **Contexto**: Incluye URLs y timestamps visibles
5. **Claridad**: AsegÃºrate que el texto sea legible

---

## ğŸ”¥ Comandos RÃ¡pidos de Limpieza (DespuÃ©s de capturar)

```bash
# Detener el job de Dataflow
gcloud dataflow jobs cancel <JOB_ID> --region=us-central1

# Opcional: Limpiar recursos (cuidado, esto borra todo)
# gcloud pubsub topics delete transactions-topic
# bq rm -r -f streaming_data_warehouse_v2
# gsutil -m rm -r gs://streaming-serverless-dataflow-staging
```

---

**Â¿Listo para empezar? Vamos paso a paso. Primero ejecuta la FASE 1 y avÃ­same cuando termines.** ğŸš€
