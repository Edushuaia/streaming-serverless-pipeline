# üéì Pipeline Serverless de Streaming en GCP - Proyecto Educativo

[![GCP](https://img.shields.io/badge/Google%20Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)](https://cloud.google.com/)
[![Apache Beam](https://img.shields.io/badge/Apache%20Beam-FF6F00?style=for-the-badge&logo=apache&logoColor=white)](https://beam.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Tests](https://img.shields.io/badge/Tests-Passing-success?style=for-the-badge)](test_pipeline.py)
[![Educational](https://img.shields.io/badge/Purpose-Educational-blue?style=for-the-badge)](README.md)

## üìö Contexto Educativo

Proyecto de investigaci√≥n y aprendizaje que explora c√≥mo las arquitecturas serverless pueden democratizar el procesamiento de datos cient√≠ficos en tiempo real. Dise√±ado como recurso educativo para estudiantes e investigadores interesados en computaci√≥n distribuida, procesamiento de datos a gran escala y tecnolog√≠as cloud.

### üî¨ Motivaci√≥n Cient√≠fico-Tecnol√≥gica

En entornos de investigaci√≥n cient√≠fica (sensores IoT, telescopios, estaciones meteorol√≥gicas, aceleradores de part√≠culas), los datos llegan de forma continua e impredecible. Este proyecto explora soluciones pr√°cticas y econ√≥micas para:

- Procesamiento de telemetr√≠a en tiempo real
- An√°lisis de datos experimentales con baja latencia
- Sistemas de monitoreo ambiental
- Alertas tempranas en investigaci√≥n

### üéØ Desaf√≠o Investigado

**¬øC√≥mo procesar flujos impredecibles de datos cient√≠ficos con baja latencia, sin infraestructura fija ni costos operativos elevados?**

## üìã Descripci√≥n

Pipeline de procesamiento de datos en tiempo real completamente serverless que demuestra principios de computaci√≥n distribuida aplicados a flujos de datos cient√≠ficos. El sistema procesa miles de eventos por segundo, los agrega en ventanas temporales y los almacena para an√°lisis posterior.

**Caracter√≠sticas Educativas:**

- ‚úÖ **C√≥digo abierto documentado** para aprendizaje
- ‚úÖ **Arquitectura replicable** para proyectos acad√©micos  
- ‚úÖ **Patrones de dise√±o** aplicables a datos cient√≠ficos
- ‚úÖ **Buenas pr√°cticas** de ingenier√≠a de software
- ‚úÖ **Testing riguroso** con cobertura > 60%
- ‚úÖ **Documentaci√≥n completa** con sitio web interactivo

**Capacidades T√©cnicas:**

- ‚úÖ **Procesamiento en tiempo real** con latencia < 5 segundos
- ‚úÖ **Autoescalado autom√°tico** basado en carga (0 ‚Üí N workers)
- ‚úÖ **Alta disponibilidad** sin gesti√≥n de infraestructura
- ‚úÖ **Agregaci√≥n por ventanas de tiempo** (Fixed Windows de 30s)
- ‚úÖ **Tolerancia a fallos** con manejo robusto de errores
- ‚úÖ **Logging estructurado** para monitoreo y debugging

## üèóÔ∏è Arquitectura

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Productor  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Cloud Pub/Sub‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇCloud Dataflow‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  BigQuery  ‚îÇ
‚îÇTransacciones‚îÇ JSON ‚îÇ    (Topic)   ‚îÇStream‚îÇ Apache Beam  ‚îÇ Batch‚îÇ  (Tabla)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                   ‚îÇ
                                                   ‚îú‚îÄ FixedWindow(30s)
                                                   ‚îú‚îÄ Agregaci√≥n SUM
                                                   ‚îî‚îÄ Autoescalado
```

### Componentes

| Servicio | Funci√≥n | Justificaci√≥n |
|----------|---------|---------------|
| **Cloud Pub/Sub** | Buffer de mensajes y desacoplamiento | Garantiza durabilidad y entrega "at-least-once" |
| **Cloud Dataflow** | Motor de procesamiento el√°stico | Autoescalado y windowing basado en Apache Beam |
| **BigQuery** | Data Warehouse columnar | Optimizado para streaming inserts y an√°lisis SQL |
| **Apache Beam** | Framework de procesamiento | Modelo unificado batch/streaming |

## üõ†Ô∏è Tecnolog√≠as

- **Google Cloud Platform (GCP)**
  - Cloud Pub/Sub
  - Cloud Dataflow
  - BigQuery
- **Apache Beam 2.x** (Python SDK)
- **Python 3.8+**
- **JSON** para formato de mensajes

## üìã Prerequisitos

### Servicios de GCP

1. Proyecto de GCP activo
2. Facturaci√≥n habilitada
3. APIs habilitadas:

   ```bash
   gcloud services enable dataflow.googleapis.com
   gcloud services enable pubsub.googleapis.com
   gcloud services enable bigquery.googleapis.com
   ```

### Herramientas locales

- **Python 3.8 o superior**
- **Google Cloud SDK** (`gcloud` CLI)
- **Git** para clonar el repositorio
- Cuenta de servicio con permisos:
  - `roles/dataflow.admin`
  - `roles/pubsub.editor`
  - `roles/bigquery.dataEditor`

### Conocimientos requeridos

- Conceptos b√°sicos de streaming de datos
- Familiaridad con GCP
- Python intermedio
- SQL para consultas en BigQuery

## üöÄ Configuraci√≥n

### 1. Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/streaming-serverless-pipeline.git
cd streaming-serverless-pipeline
```

### 2. Configurar variables de entorno

```bash
# Copiar el archivo de ejemplo
cp .env.example .env

# Editar con tu configuraci√≥n
nano .env  # o usa tu editor preferido
```

**Variables cr√≠ticas a configurar en `.env`:**

```bash
PROJECT_ID=tu-proyecto-gcp              # ID de tu proyecto GCP
REGION=us-central1                      # Regi√≥n de despliegue
PUBSUB_TOPIC_ID=transactions-topic      # Topic de Pub/Sub
BIGQUERY_DATASET_ID=streaming_data_warehouse_v2
BIGQUERY_TABLE_ID=hourly_sales_aggregation
WINDOW_SIZE_SECONDS=30                  # Tama√±o de ventana
LOG_LEVEL=INFO                          # Nivel de logging
```

### 3. Instalar dependencias

```bash
# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias de producci√≥n
pip install -r requirements.txt

# [Opcional] Instalar dependencias de desarrollo
pip install -r requirements-dev.txt
```

### 4. Configurar recursos de GCP

#### Topic y Suscripci√≥n de Pub/Sub

```bash
# Configurar PROJECT_ID
export PROJECT_ID="tu-proyecto-gcp"

# Crear Topic
gcloud pubsub topics create transactions-topic --project=${PROJECT_ID}

# Crear Suscripci√≥n (para monitoreo manual)
gcloud pubsub subscriptions create dataflow-subscription \
  --topic=transactions-topic \
  --project=${PROJECT_ID}
```

#### Bucket de staging para Dataflow

```bash
export REGION="us-central1"
export BUCKET_NAME="${PROJECT_ID}-dataflow-staging"

gsutil mb -l ${REGION} gs://${BUCKET_NAME}/
```

#### Dataset y Tabla de BigQuery

```bash
# Usar el script automatizado (recomendado)
chmod +x setup_bigquery.sh
./setup_bigquery.sh

# O manualmente:
bq mk --dataset --location=${REGION} \
  --description="Dataset para pipeline de streaming" \
  ${PROJECT_ID}:streaming_data_warehouse_v2

bq mk --table \
  --time_partitioning_field=window_start_time \
  --time_partitioning_type=DAY \
  ${PROJECT_ID}:streaming_data_warehouse_v2.hourly_sales_aggregation \
  window_start_time:TIMESTAMP:REQUIRED,\
total_transactions:INTEGER:REQUIRED,\
total_amount_sum:FLOAT:REQUIRED,\
avg_transaction_amount:FLOAT,\
max_amount:FLOAT,\
min_amount:FLOAT
```

### 5. Validar configuraci√≥n

```bash
# Validar que config.py carga correctamente
python config.py

# Debe mostrar:
# ‚úÖ Configuraci√≥n cargada correctamente
```

## üéØ Uso

### Paso 1: Iniciar el Simulador de Transacciones

El simulador genera transacciones aleatorias y las publica a Pub/Sub:

```bash
# Ejecutar el simulador
python publisher_simulator.py

# Output esperado:
# ============================================================
# INICIANDO SIMULADOR DE TRANSACCIONES
# ============================================================
# Topic: projects/tu-proyecto-gcp/topics/transactions-topic
# Intervalo: 0.5s
# Max mensajes: infinito
# Presiona Ctrl+C para detener
# ============================================================
# Estad√≠sticas: Publicados=20, Errores=0, Tasa=2.00 msg/s
```

**Configurar el simulador** (opcional):

```bash
# En .env o como variables de entorno
PUBLISHER_INTERVAL=0.5        # Intervalo entre mensajes (segundos)
PUBLISHER_MAX_MESSAGES=1000   # M√°ximo de mensajes (0 = infinito)
```

### Paso 2: Ejecutar el Pipeline

#### Modo Local (DirectRunner) - Para Testing

```bash
python dataflow_pipeline.py \
  --runner DirectRunner \
  --project ${PROJECT_ID} \
  --temp_location gs://${BUCKET_NAME}/temp/ \
  --streaming
```

‚ö†Ô∏è **Nota**: DirectRunner es para desarrollo local. NO usar en producci√≥n.

#### Modo Producci√≥n (DataflowRunner)

```bash
python dataflow_pipeline.py \
  --runner DataflowRunner \
  --project ${PROJECT_ID} \
  --region ${REGION} \
  --temp_location gs://${BUCKET_NAME}/temp/ \
  --staging_location gs://${BUCKET_NAME}/staging/ \
  --streaming \
  --max_num_workers 10 \
  --autoscaling_algorithm THROUGHPUT_BASED \
  --num_workers 2
```

**Par√°metros importantes:**

| Par√°metro | Descripci√≥n | Valor Recomendado |
|-----------|-------------|-------------------|
| `--runner` | Motor de ejecuci√≥n | `DataflowRunner` |
| `--max_num_workers` | Workers m√°ximos | `10` (desarrollo), `50+` (prod) |
| `--num_workers` | Workers iniciales | `2-5` |
| `--autoscaling_algorithm` | Algoritmo de escalado | `THROUGHPUT_BASED` |
| `--worker_machine_type` | Tipo de m√°quina | `n1-standard-2` |

### Paso 3: Monitorear el Pipeline

#### En la Consola de GCP

1. Navegar a: **Dataflow** ‚Üí **Jobs**
2. Seleccionar tu job de streaming
3. Revisar m√©tricas:
   - **System Lag**: Latencia del sistema (< 5s ideal)
   - **Data Watermark Lag**: Retraso del watermark
   - **Elements Added**: Throughput

#### Ver Logs

```bash
# Logs del pipeline en Cloud Logging
gcloud logging read "resource.type=dataflow_step" --limit 50 --format json

# Logs del simulador (local)
# Se imprimen en la terminal donde se ejecut√≥
```

### Paso 4: Consultar Resultados en BigQuery

```sql
-- Ver las √∫ltimas 10 ventanas agregadas
SELECT 
  window_start_time,
  total_transactions,
  total_amount_sum,
  avg_transaction_amount,
  max_amount,
  min_amount,
  ROUND(total_amount_sum / total_transactions, 2) AS calculated_avg
FROM `tu-proyecto-gcp.streaming_data_warehouse_v2.hourly_sales_aggregation`
ORDER BY window_start_time DESC
LIMIT 10;
```

```sql
-- An√°lisis por hora
SELECT 
  TIMESTAMP_TRUNC(window_start_time, HOUR) AS hour,
  SUM(total_transactions) AS total_txns,
  SUM(total_amount_sum) AS total_sales,
  AVG(avg_transaction_amount) AS avg_amount
FROM `tu-proyecto-gcp.streaming_data_warehouse_v2.hourly_sales_aggregation`
GROUP BY hour
ORDER BY hour DESC
LIMIT 24;
```

### Paso 5: Detener el Pipeline

```bash
# Detener el simulador
# Presionar Ctrl+C en la terminal del simulador

# Detener el pipeline de Dataflow
gcloud dataflow jobs cancel JOB_ID --region=${REGION}

# O desde la consola de GCP: Dataflow ‚Üí Jobs ‚Üí Seleccionar ‚Üí Cancel
```

## üìÇ Estructura del Proyecto

```text
streaming-serverless-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ config.py                      # ‚≠ê Configuraci√≥n centralizada
‚îú‚îÄ‚îÄ üìÑ .env.example                   # ‚≠ê Plantilla de variables de entorno
‚îú‚îÄ‚îÄ üìÑ dataflow_pipeline.py           # ‚≠ê Pipeline principal (refactorizado)
‚îú‚îÄ‚îÄ üìÑ publisher_simulator.py         # ‚≠ê Simulador de transacciones (mejorado)
‚îú‚îÄ‚îÄ üìÑ test_pipeline.py               # ‚≠ê Tests unitarios (NUEVO)
‚îÇ
‚îú‚îÄ‚îÄ üìÑ setup_bigquery.sh              # ‚≠ê Script de setup con validaciones
‚îú‚îÄ‚îÄ üìÑ requirements.txt               # ‚≠ê Dependencias de producci√≥n
‚îú‚îÄ‚îÄ üìÑ requirements-dev.txt           # ‚≠ê Dependencias de desarrollo (NUEVO)
‚îú‚îÄ‚îÄ üìÑ .gitignore                     # ‚≠ê Git ignore completo (NUEVO)
‚îÇ
‚îú‚îÄ‚îÄ üìÑ index.html                     # Portafolio web - Visi√≥n general
‚îú‚îÄ‚îÄ üìÑ pubsub.html                    # Documentaci√≥n de Cloud Pub/Sub
‚îú‚îÄ‚îÄ üìÑ dataflow.html                  # Documentaci√≥n de Cloud Dataflow
‚îú‚îÄ‚îÄ üìÑ bigquery.html                  # Documentaci√≥n de BigQuery
‚îú‚îÄ‚îÄ üìÑ apachebeam.html                # Documentaci√≥n de Apache Beam
‚îÇ
‚îú‚îÄ‚îÄ üìÅ css/
‚îÇ   ‚îî‚îÄ‚îÄ style.css                     # Estilos del portafolio
‚îÇ
‚îú‚îÄ‚îÄ üìÅ js/
‚îÇ   ‚îî‚îÄ‚îÄ main.js                       # L√≥gica de navegaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ üìÅ img/                           # Im√°genes y diagramas
‚îÇ   ‚îú‚îÄ‚îÄ architecture_diagram.png
‚îÇ   ‚îî‚îÄ‚îÄ dataflow_autoscaling.png
‚îÇ
‚îî‚îÄ‚îÄ üìÑ README.md                      # Este archivo

‚≠ê = Archivos nuevos o significativamente mejorados en v2.0
```

### Archivos Clave

| Archivo | Prop√≥sito | Novedades v2.0 |
|---------|-----------|----------------|
| **config.py** | Configuraci√≥n centralizada desde `.env` | ‚úÖ Nuevo |
| **dataflow_pipeline.py** | Pipeline de Apache Beam | ‚úÖ Logging, m√©tricas, validaci√≥n |
| **publisher_simulator.py** | Generador de datos | ‚úÖ Manejo de errores, estad√≠sticas |
| **test_pipeline.py** | Tests unitarios | ‚úÖ Nuevo (cobertura > 80%) |
| **.gitignore** | Exclusiones de Git | ‚úÖ Completo para Python/GCP |
| **setup_bigquery.sh** | Configuraci√≥n de BQ | ‚úÖ Validaciones interactivas |

## üß™ Testing

### Ejecutar Tests Unitarios

```bash
# Ejecutar todos los tests
pytest test_pipeline.py -v

# Con cobertura de c√≥digo
pytest test_pipeline.py -v --cov=dataflow_pipeline --cov-report=term-missing

# Solo un test espec√≠fico
pytest test_pipeline.py::TestParseJson::test_parse_valid_json -v
```

### Tests Incluidos

| Clase de Test | Cobertura |
|---------------|-----------|
| `TestParseJson` | Parseo de JSON, validaci√≥n, errores |
| `TestAggregateFn` | Agregaci√≥n, acumuladores, merge |
| `TestFormatForBigQuery` | Formateo, timestamps |
| `TestPipelineIntegration` | Pipeline end-to-end |

### Validaci√≥n de Configuraci√≥n

```bash
# Verificar que la configuraci√≥n carga correctamente
python config.py

# Validar formato de c√≥digo (si black est√° instalado)
black --check *.py

# Linting (si flake8 est√° instalado)
flake8 dataflow_pipeline.py publisher_simulator.py
```

## üîß Detalles T√©cnicos

### Arquitectura de Configuraci√≥n

El proyecto usa un sistema de configuraci√≥n centralizado que carga desde:

1. Archivo `.env` (prioridad alta)
2. Variables de entorno del sistema
3. Valores por defecto seguros

```python
# Uso en el c√≥digo
from config import config

project_id = config.PROJECT_ID
topic_path = config.pubsub_topic_path  # Propiedad computada
```

### L√≥gica de Windowing

El pipeline utiliza **Fixed Windows** (configurable) para agrupar transacciones:

```python
windowed_data = keyed_data | "FixedWindow" >> beam.WindowInto(
    FixedWindows(config.WINDOW_SIZE_SECONDS)  # 30s por defecto, configurable desde .env
)
```

**Ventajas de Fixed Windows:**

- Agregaciones consistentes y predecibles
- Ideal para reportes peri√≥dicos (cada 30s)
- Bajo costo computacional
- Compatible con particionamiento de BigQuery

### Funci√≥n de Agregaci√≥n Mejorada

La clase `AggregateFn` implementa `CombineFn` para calcular m√∫ltiples estad√≠sticas:

- ‚úÖ **Suma total de montos** (`total_amount_sum`)
- ‚úÖ **Conteo de transacciones** (`total_transactions`)
- ‚úÖ **Promedio calculado** (`avg_transaction_amount`)
- ‚úÖ **Monto m√°ximo** (`max_amount`)
- ‚úÖ **Monto m√≠nimo** (`min_amount`)

```python
class AggregateFn(beam.CombineFn):
    def create_accumulator(self):
        return [0.0, 0, float('-inf'), float('inf')]  # [sum, count, max, min]
    
    def add_input(self, accumulator, input):
        amount = input["amount"]
        accumulator[0] += amount          # suma
        accumulator[1] += 1               # conteo
        accumulator[2] = max(accumulator[2], amount)  # m√°ximo
        accumulator[3] = min(accumulator[3], amount)  # m√≠nimo
        return accumulator
```

### Manejo de Errores y Logging

#### Logging Estructurado

Todos los componentes usan logging estructurado con contexto:

```python
logger.error(
    f"Error de formato JSON: {str(e)}",
    extra={
        'raw_data': element[:100],
        'error_type': 'JSONDecodeError'
    }
)
```

#### M√©tricas de Beam

El pipeline expone m√©tricas para monitoreo:

```python
self.parse_success_counter = beam.metrics.Metrics.counter(
    'ParseJson', 'json_parse_success'
)
```

Ver m√©tricas en **Dataflow UI** ‚Üí Job ‚Üí Metrics

#### Estrategia de Manejo de Errores

1. **JSON Malformado**: Loguear y descartar (sin fallar el pipeline)
2. **Campos Faltantes**: Validar y descartar con warning
3. **Valores Inv√°lidos**: Rechazar montos negativos o cero
4. **Errores de Pub/Sub**: Reintentos autom√°ticos con exponential backoff

### Esquema de BigQuery Mejorado

```sql
CREATE TABLE hourly_sales_aggregation (
  window_start_time TIMESTAMP NOT NULL,      -- Inicio de la ventana
  total_transactions INT64 NOT NULL,         -- Conteo de transacciones
  total_amount_sum FLOAT64 NOT NULL,         -- Suma total
  avg_transaction_amount FLOAT64,            -- Promedio
  max_amount FLOAT64,                        -- Monto m√°ximo
  min_amount FLOAT64                         -- Monto m√≠nimo
)
PARTITION BY DATE(window_start_time);        -- Particionamiento diario
```

**Ventajas del particionamiento:**

- Consultas m√°s r√°pidas
- Costos reducidos
- Mejor organizaci√≥n temporal

## üìä Monitoreo

### M√©tricas clave en Dataflow

1. **System Lag**: Retraso en el procesamiento (objetivo: < 5s)
2. **Worker Count**: N√∫mero de workers activos
3. **Throughput**: Elementos procesados por segundo
4. **Watermark Lag**: Retraso en el watermark (baja latencia)

### Consultas de validaci√≥n

```sql
-- Verificar inserci√≥n continua (cada 30s)
SELECT 
  window_start_time,
  TIMESTAMP_DIFF(LEAD(window_start_time) OVER (ORDER BY window_start_time), 
                 window_start_time, SECOND) AS seconds_between_windows
FROM `streaming_data_warehouse_v2.hourly_sales_aggregation`
ORDER BY window_start_time DESC
LIMIT 20;

-- Detectar picos de transacciones
SELECT 
  window_start_time,
  total_transactions,
  AVG(total_transactions) OVER (ORDER BY window_start_time 
                                ROWS BETWEEN 10 PRECEDING AND CURRENT ROW) AS moving_avg
FROM `streaming_data_warehouse_v2.hourly_sales_aggregation`
WHERE total_transactions > (SELECT AVG(total_transactions) * 1.5 FROM `streaming_data_warehouse_v2.hourly_sales_aggregation`)
ORDER BY window_start_time DESC;
```

## üêõ Resoluci√≥n de Problemas

### Error: "Table not found"

**Causa**: La tabla de BigQuery no existe o est√° en diferente regi√≥n.

**Soluci√≥n**:

```bash
./setup_bigquery.sh
# Verificar que la regi√≥n coincida con Dataflow (us-central1)
```

### Error: "Permission denied"

**Causa**: La cuenta de servicio no tiene permisos suficientes.

**Soluci√≥n**:

```bash
gcloud projects add-iam-policy-binding ${PROJECT_ID} \
  --member="serviceAccount:TU_SERVICE_ACCOUNT@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/dataflow.admin"
```

### Pipeline no escala

**Causa**: L√≠mite de workers o configuraci√≥n de autoscaling.

**Soluci√≥n**:

- Aumentar `--max_num_workers`
- Verificar cuotas de Compute Engine
- Usar `--autoscaling_algorithm THROUGHPUT_BASED`

### Latencia alta

**Causa**: Watermark retrasado o configuraci√≥n de ventanas.

**Soluci√≥n**:

- Reducir tama√±o de ventana (15s en lugar de 30s)
- Aumentar workers
- Verificar backlog de Pub/Sub

## üìà Mejoras Futuras

- [ ] Implementar Dead Letter Queue para errores
- [ ] Agregar alertas con Cloud Monitoring
- [ ] Dashboard en Looker Studio
- [ ] Pipeline de reentrenamiento de ML
- [ ] Detecci√≥n de anomal√≠as en tiempo real
- [ ] Multi-regi√≥n para alta disponibilidad
- [ ] Compresi√≥n de datos en Pub/Sub
- [ ] Particionamiento de BigQuery por timestamp

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto es de c√≥digo abierto y est√° disponible bajo la [Licencia MIT](LICENSE).

## üë§ Autor

### Portafolio de Ingenier√≠a de Datos

- Website: [Ver Demo](index.html)
- GitHub: [@tu-usuario](https://github.com/tu-usuario)

## üôè Agradecimientos

- [Apache Beam Documentation](https://beam.apache.org/documentation/)
- [Google Cloud Dataflow](https://cloud.google.com/dataflow/docs)
- [Google Cloud Pub/Sub Best Practices](https://cloud.google.com/pubsub/docs/best-practices)

---

‚≠ê **Si este proyecto te result√≥ √∫til, considera darle una estrella en GitHub**
