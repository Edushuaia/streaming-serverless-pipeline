# Pipeline Serverless de Streaming en GCP

[![GCP](https://img.shields.io/badge/Google%20Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)](https://cloud.google.com/)
[![Apache Beam](https://img.shields.io/badge/Apache%20Beam-FF6F00?style=for-the-badge&logo=apache&logoColor=white)](https://beam.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Tests](https://img.shields.io/badge/Tests-Passing-success?style=for-the-badge)](test_pipeline.py)
[![Code Style](https://img.shields.io/badge/Code%20Style-Black-000000?style=for-the-badge)](https://github.com/psf/black)

## ğŸ“‹ DescripciÃ³n

Pipeline de procesamiento de datos en tiempo real completamente serverless que analiza flujos de transacciones financieras utilizando servicios gestionados de Google Cloud Platform. El sistema procesa miles de transacciones por segundo, las agrega en ventanas de tiempo de 30 segundos y las almacena para anÃ¡lisis posterior.

**CaracterÃ­sticas principales:**

- âœ… **Procesamiento en tiempo real** con latencia < 5 segundos
- âœ… **Autoescalado automÃ¡tico** basado en carga
- âœ… **Alta disponibilidad** sin gestiÃ³n de infraestructura
- âœ… **AgregaciÃ³n por ventanas de tiempo** (Fixed Windows)
- âœ… **Tolerancia a fallos** con manejo robusto de errores
- âœ… **Logging estructurado** para monitoreo y debugging
- âœ… **Tests unitarios** con cobertura > 80%
- âœ… **ConfiguraciÃ³n centralizada** desde variables de entorno
- âœ… **MÃ©tricas de Beam** para observabilidad

## ğŸ—ï¸ Arquitectura

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Productor  â”‚â”€â”€â”€â”€â”€â–¶â”‚ Cloud Pub/Subâ”‚â”€â”€â”€â”€â”€â–¶â”‚Cloud Dataflowâ”‚â”€â”€â”€â”€â”€â–¶â”‚  BigQuery  â”‚
â”‚Transaccionesâ”‚ JSON â”‚    (Topic)   â”‚Streamâ”‚ Apache Beam  â”‚ Batchâ”‚  (Tabla)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â”œâ”€ FixedWindow(30s)
                                                   â”œâ”€ AgregaciÃ³n SUM
                                                   â””â”€ Autoescalado
```

### Componentes

| Servicio | FunciÃ³n | JustificaciÃ³n |
|----------|---------|---------------|
| **Cloud Pub/Sub** | Buffer de mensajes y desacoplamiento | Garantiza durabilidad y entrega "at-least-once" |
| **Cloud Dataflow** | Motor de procesamiento elÃ¡stico | Autoescalado y windowing basado en Apache Beam |
| **BigQuery** | Data Warehouse columnar | Optimizado para streaming inserts y anÃ¡lisis SQL |
| **Apache Beam** | Framework de procesamiento | Modelo unificado batch/streaming |

## ğŸ› ï¸ TecnologÃ­as

- **Google Cloud Platform (GCP)**
  - Cloud Pub/Sub
  - Cloud Dataflow
  - BigQuery
- **Apache Beam 2.x** (Python SDK)
- **Python 3.8+**
- **JSON** para formato de mensajes

## ğŸ“‹ Prerequisitos

### Servicios de GCP

1. Proyecto de GCP activo
2. FacturaciÃ³n habilitada
3. APIs habilitadas:

   ```bash
   gcloud services enable dataflow.googleapis.com
   gcloud services enable pubsub.googleapis.com
   gcloud services enable bigquery.googleapis.com
   ```

### Herramientas locales

- **Python 3.8 o superior**
- **Google Cloud SDK** (`gcloud` CLI)
- **Git** para clonar el repositorio
- Cuenta de servicio con permisos:
  - `roles/dataflow.admin`
  - `roles/pubsub.editor`
  - `roles/bigquery.dataEditor`

### Conocimientos requeridos

- Conceptos bÃ¡sicos de streaming de datos
- Familiaridad con GCP
- Python intermedio
- SQL para consultas en BigQuery

## ğŸš€ ConfiguraciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/streaming-serverless-pipeline.git
cd streaming-serverless-pipeline
```

### 2. Configurar variables de entorno

```bash
# Copiar el archivo de ejemplo
cp .env.example .env

# Editar con tu configuraciÃ³n
nano .env  # o usa tu editor preferido
```

**Variables crÃ­ticas a configurar en `.env`:**

```bash
PROJECT_ID=tu-proyecto-gcp              # ID de tu proyecto GCP
REGION=us-central1                      # RegiÃ³n de despliegue
PUBSUB_TOPIC_ID=transactions-topic      # Topic de Pub/Sub
BIGQUERY_DATASET_ID=streaming_data_warehouse_v2
BIGQUERY_TABLE_ID=hourly_sales_aggregation
WINDOW_SIZE_SECONDS=30                  # TamaÃ±o de ventana
LOG_LEVEL=INFO                          # Nivel de logging
```

### 3. Instalar dependencias

```bash
# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias de producciÃ³n
pip install -r requirements.txt

# [Opcional] Instalar dependencias de desarrollo
pip install -r requirements-dev.txt
```

### 4. Configurar recursos de GCP

#### Topic y SuscripciÃ³n de Pub/Sub

```bash
# Configurar PROJECT_ID
export PROJECT_ID="tu-proyecto-gcp"

# Crear Topic
gcloud pubsub topics create transactions-topic --project=${PROJECT_ID}

# Crear SuscripciÃ³n (para monitoreo manual)
gcloud pubsub subscriptions create dataflow-subscription \
  --topic=transactions-topic \
  --project=${PROJECT_ID}
```

#### Bucket de staging para Dataflow

```bash
export REGION="us-central1"
export BUCKET_NAME="${PROJECT_ID}-dataflow-staging"

gsutil mb -l ${REGION} gs://${BUCKET_NAME}/
```

#### Dataset y Tabla de BigQuery

```bash
# Usar el script automatizado (recomendado)
chmod +x setup_bigquery.sh
./setup_bigquery.sh

# O manualmente:
bq mk --dataset --location=${REGION} \
  --description="Dataset para pipeline de streaming" \
  ${PROJECT_ID}:streaming_data_warehouse_v2

bq mk --table \
  --time_partitioning_field=window_start_time \
  --time_partitioning_type=DAY \
  ${PROJECT_ID}:streaming_data_warehouse_v2.hourly_sales_aggregation \
  window_start_time:TIMESTAMP:REQUIRED,\
total_transactions:INTEGER:REQUIRED,\
total_amount_sum:FLOAT:REQUIRED,\
avg_transaction_amount:FLOAT,\
max_amount:FLOAT,\
min_amount:FLOAT
```

### 5. Validar configuraciÃ³n

```bash
# Validar que config.py carga correctamente
python config.py

# Debe mostrar:
# âœ… ConfiguraciÃ³n cargada correctamente
```

## ğŸ¯ Uso

### Paso 1: Iniciar el Simulador de Transacciones

El simulador genera transacciones aleatorias y las publica a Pub/Sub:

```bash
# Ejecutar el simulador
python publisher_simulator.py

# Output esperado:
# ============================================================
# INICIANDO SIMULADOR DE TRANSACCIONES
# ============================================================
# Topic: projects/tu-proyecto-gcp/topics/transactions-topic
# Intervalo: 0.5s
# Max mensajes: infinito
# Presiona Ctrl+C para detener
# ============================================================
# EstadÃ­sticas: Publicados=20, Errores=0, Tasa=2.00 msg/s
```

**Configurar el simulador** (opcional):

```bash
# En .env o como variables de entorno
PUBLISHER_INTERVAL=0.5        # Intervalo entre mensajes (segundos)
PUBLISHER_MAX_MESSAGES=1000   # MÃ¡ximo de mensajes (0 = infinito)
```

### Paso 2: Ejecutar el Pipeline

#### Modo Local (DirectRunner) - Para Testing

```bash
python dataflow_pipeline.py \
  --runner DirectRunner \
  --project ${PROJECT_ID} \
  --temp_location gs://${BUCKET_NAME}/temp/ \
  --streaming
```

âš ï¸ **Nota**: DirectRunner es para desarrollo local. NO usar en producciÃ³n.

#### Modo ProducciÃ³n (DataflowRunner)

```bash
python dataflow_pipeline.py \
  --runner DataflowRunner \
  --project ${PROJECT_ID} \
  --region ${REGION} \
  --temp_location gs://${BUCKET_NAME}/temp/ \
  --staging_location gs://${BUCKET_NAME}/staging/ \
  --streaming \
  --max_num_workers 10 \
  --autoscaling_algorithm THROUGHPUT_BASED \
  --num_workers 2
```

**ParÃ¡metros importantes:**

| ParÃ¡metro | DescripciÃ³n | Valor Recomendado |
|-----------|-------------|-------------------|
| `--runner` | Motor de ejecuciÃ³n | `DataflowRunner` |
| `--max_num_workers` | Workers mÃ¡ximos | `10` (desarrollo), `50+` (prod) |
| `--num_workers` | Workers iniciales | `2-5` |
| `--autoscaling_algorithm` | Algoritmo de escalado | `THROUGHPUT_BASED` |
| `--worker_machine_type` | Tipo de mÃ¡quina | `n1-standard-2` |

### Paso 3: Monitorear el Pipeline

#### En la Consola de GCP

1. Navegar a: **Dataflow** â†’ **Jobs**
2. Seleccionar tu job de streaming
3. Revisar mÃ©tricas:
   - **System Lag**: Latencia del sistema (< 5s ideal)
   - **Data Watermark Lag**: Retraso del watermark
   - **Elements Added**: Throughput

#### Ver Logs

```bash
# Logs del pipeline en Cloud Logging
gcloud logging read "resource.type=dataflow_step" --limit 50 --format json

# Logs del simulador (local)
# Se imprimen en la terminal donde se ejecutÃ³
```

### Paso 4: Consultar Resultados en BigQuery

```sql
-- Ver las Ãºltimas 10 ventanas agregadas
SELECT 
  window_start_time,
  total_transactions,
  total_amount_sum,
  avg_transaction_amount,
  max_amount,
  min_amount,
  ROUND(total_amount_sum / total_transactions, 2) AS calculated_avg
FROM `tu-proyecto-gcp.streaming_data_warehouse_v2.hourly_sales_aggregation`
ORDER BY window_start_time DESC
LIMIT 10;
```

```sql
-- AnÃ¡lisis por hora
SELECT 
  TIMESTAMP_TRUNC(window_start_time, HOUR) AS hour,
  SUM(total_transactions) AS total_txns,
  SUM(total_amount_sum) AS total_sales,
  AVG(avg_transaction_amount) AS avg_amount
FROM `tu-proyecto-gcp.streaming_data_warehouse_v2.hourly_sales_aggregation`
GROUP BY hour
ORDER BY hour DESC
LIMIT 24;
```

### Paso 5: Detener el Pipeline

```bash
# Detener el simulador
# Presionar Ctrl+C en la terminal del simulador

# Detener el pipeline de Dataflow
gcloud dataflow jobs cancel JOB_ID --region=${REGION}

# O desde la consola de GCP: Dataflow â†’ Jobs â†’ Seleccionar â†’ Cancel
```

## ğŸ“‚ Estructura del Proyecto

```text
streaming-serverless-pipeline/
â”‚
â”œâ”€â”€ ğŸ“„ config.py                      # â­ ConfiguraciÃ³n centralizada
â”œâ”€â”€ ğŸ“„ .env.example                   # â­ Plantilla de variables de entorno
â”œâ”€â”€ ğŸ“„ dataflow_pipeline.py           # â­ Pipeline principal (refactorizado)
â”œâ”€â”€ ğŸ“„ publisher_simulator.py         # â­ Simulador de transacciones (mejorado)
â”œâ”€â”€ ğŸ“„ test_pipeline.py               # â­ Tests unitarios (NUEVO)
â”‚
â”œâ”€â”€ ğŸ“„ setup_bigquery.sh              # â­ Script de setup con validaciones
â”œâ”€â”€ ğŸ“„ requirements.txt               # â­ Dependencias de producciÃ³n
â”œâ”€â”€ ğŸ“„ requirements-dev.txt           # â­ Dependencias de desarrollo (NUEVO)
â”œâ”€â”€ ğŸ“„ .gitignore                     # â­ Git ignore completo (NUEVO)
â”‚
â”œâ”€â”€ ğŸ“„ index.html                     # Portafolio web - VisiÃ³n general
â”œâ”€â”€ ğŸ“„ pubsub.html                    # DocumentaciÃ³n de Cloud Pub/Sub
â”œâ”€â”€ ğŸ“„ dataflow.html                  # DocumentaciÃ³n de Cloud Dataflow
â”œâ”€â”€ ğŸ“„ bigquery.html                  # DocumentaciÃ³n de BigQuery
â”œâ”€â”€ ğŸ“„ apachebeam.html                # DocumentaciÃ³n de Apache Beam
â”‚
â”œâ”€â”€ ğŸ“ css/
â”‚   â””â”€â”€ style.css                     # Estilos del portafolio
â”‚
â”œâ”€â”€ ğŸ“ js/
â”‚   â””â”€â”€ main.js                       # LÃ³gica de navegaciÃ³n
â”‚
â”œâ”€â”€ ğŸ“ img/                           # ImÃ¡genes y diagramas
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â””â”€â”€ dataflow_autoscaling.png
â”‚
â””â”€â”€ ğŸ“„ README.md                      # Este archivo

â­ = Archivos nuevos o significativamente mejorados en v2.0
```

### Archivos Clave

| Archivo | PropÃ³sito | Novedades v2.0 |
|---------|-----------|----------------|
| **config.py** | ConfiguraciÃ³n centralizada desde `.env` | âœ… Nuevo |
| **dataflow_pipeline.py** | Pipeline de Apache Beam | âœ… Logging, mÃ©tricas, validaciÃ³n |
| **publisher_simulator.py** | Generador de datos | âœ… Manejo de errores, estadÃ­sticas |
| **test_pipeline.py** | Tests unitarios | âœ… Nuevo (cobertura > 80%) |
| **.gitignore** | Exclusiones de Git | âœ… Completo para Python/GCP |
| **setup_bigquery.sh** | ConfiguraciÃ³n de BQ | âœ… Validaciones interactivas |

## ğŸ§ª Testing

### Ejecutar Tests Unitarios

```bash
# Ejecutar todos los tests
pytest test_pipeline.py -v

# Con cobertura de cÃ³digo
pytest test_pipeline.py -v --cov=dataflow_pipeline --cov-report=term-missing

# Solo un test especÃ­fico
pytest test_pipeline.py::TestParseJson::test_parse_valid_json -v
```

### Tests Incluidos

| Clase de Test | Cobertura |
|---------------|-----------|
| `TestParseJson` | Parseo de JSON, validaciÃ³n, errores |
| `TestAggregateFn` | AgregaciÃ³n, acumuladores, merge |
| `TestFormatForBigQuery` | Formateo, timestamps |
| `TestPipelineIntegration` | Pipeline end-to-end |

### ValidaciÃ³n de ConfiguraciÃ³n

```bash
# Verificar que la configuraciÃ³n carga correctamente
python config.py

# Validar formato de cÃ³digo (si black estÃ¡ instalado)
black --check *.py

# Linting (si flake8 estÃ¡ instalado)
flake8 dataflow_pipeline.py publisher_simulator.py
```

## ğŸ”§ Detalles TÃ©cnicos

### Arquitectura de ConfiguraciÃ³n

El proyecto usa un sistema de configuraciÃ³n centralizado que carga desde:

1. Archivo `.env` (prioridad alta)
2. Variables de entorno del sistema
3. Valores por defecto seguros

```python
# Uso en el cÃ³digo
from config import config

project_id = config.PROJECT_ID
topic_path = config.pubsub_topic_path  # Propiedad computada
```

### LÃ³gica de Windowing

El pipeline utiliza **Fixed Windows** (configurable) para agrupar transacciones:

```python
windowed_data = keyed_data | "FixedWindow" >> beam.WindowInto(
    FixedWindows(config.WINDOW_SIZE_SECONDS)  # 30s por defecto, configurable desde .env
)
```

**Ventajas de Fixed Windows:**

- Agregaciones consistentes y predecibles
- Ideal para reportes periÃ³dicos (cada 30s)
- Bajo costo computacional
- Compatible con particionamiento de BigQuery

### FunciÃ³n de AgregaciÃ³n Mejorada

La clase `AggregateFn` implementa `CombineFn` para calcular mÃºltiples estadÃ­sticas:

- âœ… **Suma total de montos** (`total_amount_sum`)
- âœ… **Conteo de transacciones** (`total_transactions`)
- âœ… **Promedio calculado** (`avg_transaction_amount`)
- âœ… **Monto mÃ¡ximo** (`max_amount`)
- âœ… **Monto mÃ­nimo** (`min_amount`)

```python
class AggregateFn(beam.CombineFn):
    def create_accumulator(self):
        return [0.0, 0, float('-inf'), float('inf')]  # [sum, count, max, min]
    
    def add_input(self, accumulator, input):
        amount = input["amount"]
        accumulator[0] += amount          # suma
        accumulator[1] += 1               # conteo
        accumulator[2] = max(accumulator[2], amount)  # mÃ¡ximo
        accumulator[3] = min(accumulator[3], amount)  # mÃ­nimo
        return accumulator
```

### Manejo de Errores y Logging

#### Logging Estructurado

Todos los componentes usan logging estructurado con contexto:

```python
logger.error(
    f"Error de formato JSON: {str(e)}",
    extra={
        'raw_data': element[:100],
        'error_type': 'JSONDecodeError'
    }
)
```

#### MÃ©tricas de Beam

El pipeline expone mÃ©tricas para monitoreo:

```python
self.parse_success_counter = beam.metrics.Metrics.counter(
    'ParseJson', 'json_parse_success'
)
```

Ver mÃ©tricas en **Dataflow UI** â†’ Job â†’ Metrics

#### Estrategia de Manejo de Errores

1. **JSON Malformado**: Loguear y descartar (sin fallar el pipeline)
2. **Campos Faltantes**: Validar y descartar con warning
3. **Valores InvÃ¡lidos**: Rechazar montos negativos o cero
4. **Errores de Pub/Sub**: Reintentos automÃ¡ticos con exponential backoff

### Esquema de BigQuery Mejorado

```sql
CREATE TABLE hourly_sales_aggregation (
  window_start_time TIMESTAMP NOT NULL,      -- Inicio de la ventana
  total_transactions INT64 NOT NULL,         -- Conteo de transacciones
  total_amount_sum FLOAT64 NOT NULL,         -- Suma total
  avg_transaction_amount FLOAT64,            -- Promedio
  max_amount FLOAT64,                        -- Monto mÃ¡ximo
  min_amount FLOAT64                         -- Monto mÃ­nimo
)
PARTITION BY DATE(window_start_time);        -- Particionamiento diario
```

**Ventajas del particionamiento:**

- Consultas mÃ¡s rÃ¡pidas
- Costos reducidos
- Mejor organizaciÃ³n temporal

## ğŸ“Š Monitoreo

### MÃ©tricas clave en Dataflow

1. **System Lag**: Retraso en el procesamiento (objetivo: < 5s)
2. **Worker Count**: NÃºmero de workers activos
3. **Throughput**: Elementos procesados por segundo
4. **Watermark Lag**: Retraso en el watermark (baja latencia)

### Consultas de validaciÃ³n

```sql
-- Verificar inserciÃ³n continua (cada 30s)
SELECT 
  window_start_time,
  TIMESTAMP_DIFF(LEAD(window_start_time) OVER (ORDER BY window_start_time), 
                 window_start_time, SECOND) AS seconds_between_windows
FROM `streaming_data_warehouse_v2.hourly_sales_aggregation`
ORDER BY window_start_time DESC
LIMIT 20;

-- Detectar picos de transacciones
SELECT 
  window_start_time,
  total_transactions,
  AVG(total_transactions) OVER (ORDER BY window_start_time 
                                ROWS BETWEEN 10 PRECEDING AND CURRENT ROW) AS moving_avg
FROM `streaming_data_warehouse_v2.hourly_sales_aggregation`
WHERE total_transactions > (SELECT AVG(total_transactions) * 1.5 FROM `streaming_data_warehouse_v2.hourly_sales_aggregation`)
ORDER BY window_start_time DESC;
```

## ğŸ› ResoluciÃ³n de Problemas

### Error: "Table not found"

**Causa**: La tabla de BigQuery no existe o estÃ¡ en diferente regiÃ³n.

**SoluciÃ³n**:

```bash
./setup_bigquery.sh
# Verificar que la regiÃ³n coincida con Dataflow (us-central1)
```

### Error: "Permission denied"

**Causa**: La cuenta de servicio no tiene permisos suficientes.

**SoluciÃ³n**:

```bash
gcloud projects add-iam-policy-binding ${PROJECT_ID} \
  --member="serviceAccount:TU_SERVICE_ACCOUNT@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/dataflow.admin"
```

### Pipeline no escala

**Causa**: LÃ­mite de workers o configuraciÃ³n de autoscaling.

**SoluciÃ³n**:

- Aumentar `--max_num_workers`
- Verificar cuotas de Compute Engine
- Usar `--autoscaling_algorithm THROUGHPUT_BASED`

### Latencia alta

**Causa**: Watermark retrasado o configuraciÃ³n de ventanas.

**SoluciÃ³n**:

- Reducir tamaÃ±o de ventana (15s en lugar de 30s)
- Aumentar workers
- Verificar backlog de Pub/Sub

## ğŸ“ˆ Mejoras Futuras

- [ ] Implementar Dead Letter Queue para errores
- [ ] Agregar alertas con Cloud Monitoring
- [ ] Dashboard en Looker Studio
- [ ] Pipeline de reentrenamiento de ML
- [ ] DetecciÃ³n de anomalÃ­as en tiempo real
- [ ] Multi-regiÃ³n para alta disponibilidad
- [ ] CompresiÃ³n de datos en Pub/Sub
- [ ] Particionamiento de BigQuery por timestamp

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la [Licencia MIT](LICENSE).

## ğŸ‘¤ Autor

### Portafolio de IngenierÃ­a de Datos

- Website: [Ver Demo](index.html)
- GitHub: [@tu-usuario](https://github.com/tu-usuario)

## ğŸ™ Agradecimientos

- [Apache Beam Documentation](https://beam.apache.org/documentation/)
- [Google Cloud Dataflow](https://cloud.google.com/dataflow/docs)
- [Google Cloud Pub/Sub Best Practices](https://cloud.google.com/pubsub/docs/best-practices)

---

â­ **Si este proyecto te resultÃ³ Ãºtil, considera darle una estrella en GitHub**
