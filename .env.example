# ==============================================================================
# ARCHIVO DE EJEMPLO DE VARIABLES DE ENTORNO
# ==============================================================================
# 
# Instrucciones:
# 1. Copia este archivo y renómbralo a ".env"
# 2. Completa los valores con tu configuración específica
# 3. NUNCA comitees el archivo .env (está en .gitignore)
#
# Uso: El módulo config.py cargará automáticamente estas variables
# ==============================================================================

# =========================
# CONFIGURACIÓN DE GCP
# =========================
# ID de tu proyecto en Google Cloud Platform
# Obtener: gcloud config get-value project
PROJECT_ID=tu-proyecto-gcp

# Región donde desplegar los recursos (debe soportar Dataflow)
# Opciones comunes: us-central1, us-east1, europe-west1
REGION=us-central1

# =========================
# CLOUD STORAGE
# =========================
# Nombre del bucket de GCS para staging de Dataflow
# Se creará automáticamente si no existe
BUCKET_NAME=tu-proyecto-gcp-dataflow-staging

# =========================
# CLOUD PUB/SUB
# =========================
# ID del Topic donde se publicarán las transacciones
PUBSUB_TOPIC_ID=transactions-topic

# ID de la Suscripción que consumirá Dataflow
PUBSUB_SUBSCRIPTION_ID=dataflow-subscription

# =========================
# BIGQUERY
# =========================
# ID del dataset donde se almacenarán los resultados
BIGQUERY_DATASET_ID=streaming_data_warehouse_v2

# ID de la tabla de resultados agregados
BIGQUERY_TABLE_ID=hourly_sales_aggregation

# =========================
# CONFIGURACIÓN DEL PIPELINE
# =========================
# Tamaño de la ventana de agregación en segundos
# Valor recomendado: 30-60 segundos para tiempo real
WINDOW_SIZE_SECONDS=30

# =========================
# CONFIGURACIÓN DE DATAFLOW
# =========================
# Número máximo de workers que Dataflow puede escalar
# Desarrollo: 2-5, Producción: 10-100
MAX_NUM_WORKERS=10

# Algoritmo de autoescalado
# Opciones: THROUGHPUT_BASED, NONE
AUTOSCALING_ALGORITHM=THROUGHPUT_BASED

# =========================
# LOGGING Y DEBUGGING
# =========================
# Nivel de logging
# Opciones: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =========================
# ENTORNO
# =========================
# Identificador del entorno de ejecución
# Opciones: development, staging, production
ENVIRONMENT=development

# =========================
# CONFIGURACIÓN DEL PUBLISHER (SIMULADOR)
# =========================
# Intervalo entre publicaciones en segundos
PUBLISHER_INTERVAL=0.5

# Número de mensajes a publicar (0 = infinito)
PUBLISHER_MAX_MESSAGES=0

# =========================
# NOTAS ADICIONALES
# =========================
# 
# Para obtener tu PROJECT_ID actual:
#   gcloud config get-value project
#
# Para crear los recursos necesarios:
#   ./setup_bigquery.sh
#   gcloud pubsub topics create transactions-topic
#   gsutil mb -l ${REGION} gs://${BUCKET_NAME}/
#
# Para validar la configuración:
#   python config.py
#
